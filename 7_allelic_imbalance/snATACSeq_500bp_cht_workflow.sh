#!/bin/bash
#SBATCH --time=10:00:00
#SBATCH --account=def-account
#SBATCH --mem=10G
#SBATCH --cpus-per-task=1
#SBATCH --output=%x-%j.out
#SBATCH --job-name=snatac_all_waspNODUP_500bp

#load env
source ~/scratch/WASP_allelic_imbalance/env/bin/activate

#load modules
module load hdf5/1.14.2

# Set these environment vars to point to
# your local installation of WASP
WASP=~/scratch/WASP_allelic_imbalance/WASP
DATA_DIR=$WASP/example_snATAC_all_waspNODUP_500bp
GENOTYPES=/project/Genotyping_Multiple_Sets/set_1_2_3_22_4_5_combined_analysis/8_post_imputation_filtering_v5/anjali_subjects_allelic_imbalance/hg38_split_chr_vcfs/
#
# Convert SNP files to HDF5 format. This program can be used
# on output from impute2 or on VCF files. Note that
# you do not need to repeat this step here if it was already
# done in the mapping pipeline.
#
$WASP/snp2h5/snp2h5 --chrom $DATA_DIR/chromInfo.hg38.txt \
	      --format vcf \
	      --geno_prob $DATA_DIR/geno_probs.h5 \
	      --snp_index $DATA_DIR/snp_index.h5 \
	      --snp_tab $DATA_DIR/snp_tab.h5 \
	      --haplotype $DATA_DIR/haps.h5 \
	      --samples $DATA_DIR/genotypes/atac_samples.txt \
	      $GENOTYPES/filtered_split.chr*.vcf.gz

#
# Convert FASTA files to HDF5 format.
# Note the HDF5 sequence files are only used for GC content
# correction part of CHT. This step can be omitted if
# GC-content correction is not used.
#
# The chr*.fa.gz files are the fasta files for the reference genome
# and can be downloaded from a genome browser (e.g.:
# http://hgdownload.soe.ucsc.edu/goldenPath/hg38/chromosomes/)
#
$WASP/snp2h5/fasta2h5 --chrom $DATA_DIR/chromInfo.hg38.txt \
	--seq $DATA_DIR/seq.h5 \
	~/scratch/WASP_allelic_imbalance/reference_genomes/hg38/chr*.fa.gz


#exit

# loop over all individuals in samples file
ATAC_SAMPLES_FILE=$DATA_DIR/atac_samples.txt
ALL_SAMPLES_FILE=$DATA_DIR/atac_samples_total.txt

for INDIVIDUAL in $(cat $ATAC_SAMPLES_FILE)
do
   echo $INDIVIDUAL

   
   # read BAM files for this individual and write read counts to
   # HDF5 files
   
python $WASP/CHT/bam2h5.py --chrom $DATA_DIR/chromInfo.hg38.txt \
	--snp_index $DATA_DIR/snp_index.h5 \
	--snp_tab $DATA_DIR/snp_tab.h5 \
	--haplotype $DATA_DIR/haps.h5 \
	--individual $INDIVIDUAL \
	--ref_as_counts $DATA_DIR/ref_as_counts.$INDIVIDUAL.h5 \
	--alt_as_counts $DATA_DIR/alt_as_counts.$INDIVIDUAL.h5 \
	--other_as_counts $DATA_DIR/other_as_counts.$INDIVIDUAL.h5 \
	--read_counts $DATA_DIR/read_counts.$INDIVIDUAL.h5 \
	~/scratch/WASP_allelic_imbalance/WASP/WASP_BAM_NODUP/snATAC/sorted_bam/$INDIVIDUAL.bam

done


#
# Make a list of target regions in ChIP-seq peaks and associated SNPs
# to test with the CHT (written to chr22.peaks.txt.gz). The provided
# get_target_regions.py script can be used to identify target regions
# and test SNPs that match specific criteria, (e.g. minimum number of
# heterozygous individuals and total number of allele-specific reads
# in target region). The file containing target regions and test SNPs can
# also be generated by the user (for example if a specific set of
# target regions and test SNPs are to be tested).
#
python $WASP/CHT/get_target_regions.py \
     --target_region_size 500 \
     --min_as_count 1 \
     --min_read_count 1 \
     --min_het_count 1 \
     --min_minor_allele_count 1\
     --chrom $DATA_DIR/chromInfo.hg38.txt \
     --read_count_dir $DATA_DIR \
     --individuals $ATAC_SAMPLES_FILE \
     --samples $ALL_SAMPLES_FILE \
     --snp_tab $DATA_DIR/snp_tab.h5 \
     --snp_index $DATA_DIR/snp_index.h5 \
     --haplotype $DATA_DIR/haps.h5 \
     --output_file $DATA_DIR/samples.allchrom.peaks.txt.gz

#exit

for INDIVIDUAL in $(cat $ATAC_SAMPLES_FILE)
do
# #    #
#    # create CHT input file for this individual
# #    #
   python $WASP/CHT/extract_haplotype_read_counts.py \
      --chrom $DATA_DIR/chromInfo.hg38.txt \
      --snp_index $DATA_DIR/snp_index.h5 \
     --snp_tab $DATA_DIR/snp_tab.h5 \
      --geno_prob $DATA_DIR/geno_probs.h5 \
      --haplotype $DATA_DIR/haps.h5 \
      --samples $ALL_SAMPLES_FILE \
      --individual $INDIVIDUAL \
      --ref_as_counts $DATA_DIR/ref_as_counts.$INDIVIDUAL.h5 \
     --alt_as_counts $DATA_DIR/alt_as_counts.$INDIVIDUAL.h5 \
      --other_as_counts $DATA_DIR/other_as_counts.$INDIVIDUAL.h5 \
      --read_counts $DATA_DIR/read_counts.$INDIVIDUAL.h5 \
      $DATA_DIR/peaks_snatac_500bp_filtered.txt.gz \
      | gzip > $DATA_DIR/haplotype_read_counts.$INDIVIDUAL.txt.gz

done



# Adjust read counts in CHT files by modeling
# relationship between read depth and GC content & peakiness
# in each sample.
# (first make files containing lists of input and output files)
#

IN_FILE=$DATA_DIR/input_files.txt
OUT_FILE=$DATA_DIR/output_files.txt
ls $DATA_DIR/haplotype_read_counts* | grep -v adjusted > $IN_FILE
cat $IN_FILE | sed 's/.txt/.adjusted.txt/' >  $OUT_FILE

python $WASP/CHT/update_total_depth_clip.py --seq $DATA_DIR/seq.h5 $IN_FILE $OUT_FILE
#exit


# Adjust heterozygote probabilities in CHT files to account for
# possible genotyping errors. Total counts of reference and
# alternative alleles are used to adjust the probability. In
# this example we just provide the same H3K27ac read counts, however
# you could also use read counts combined across many different
# experiments or (perhaps ideally) from DNA sequencing.
#
for INDIVIDUAL in $(cat $ATAC_SAMPLES_FILE)
do
   IN_FILE=$DATA_DIR/haplotype_read_counts.$INDIVIDUAL.adjusted.txt.gz
   OUT_FILE=$DATA_DIR/haplotype_read_counts.$INDIVIDUAL.adjusted.hetp.txt.gz
    
   python $WASP/CHT/update_het_probs.py \
	   --ref_as_counts $DATA_DIR/ref_as_counts.$INDIVIDUAL.h5  \
	   --alt_as_counts $DATA_DIR/alt_as_counts.$INDIVIDUAL.h5 \
	   $IN_FILE $OUT_FILE    
done


CHT_IN_FILE=$DATA_DIR/cht_input_file.txt
ls $DATA_DIR/haplotype_read_counts*.adjusted.hetp.txt.gz > $CHT_IN_FILE

#
# Estimate overdispersion parameters for allele-specific test (beta binomial)
#
OUT_FILE=$DATA_DIR/cht_as_coef.txt
python $WASP/CHT/fit_as_coefficients.py $CHT_IN_FILE $OUT_FILE


#
# Estimate overdispersion parameters for association test (beta-negative binomial)
#
OUT_FILE=$DATA_DIR/cht_bnb_coef.txt
python $WASP/CHT/fit_bnb_coefficients.py --min_counts 1 --min_as_counts 1 $CHT_IN_FILE $OUT_FILE


#
# run combined haplotype test
#
OUT_FILE=$DATA_DIR/cht_results.txt
python $WASP/CHT/combined_test.py --min_as_counts 1 \
      --bnb_disp $DATA_DIR/cht_bnb_coef.txt \
      --as_disp $DATA_DIR/cht_as_coef.txt \
      $CHT_IN_FILE $OUT_FILE


# Optionally, principcal component loadings can be used as covariates
# by the CHT. An example of how to perform PCA and obtain principal
# component loadings is provided in the file example_data/H3K27ac/get_PCs.R
# Note that we only recommend using PCs as covariates in when sample
# sizes are fairly large (e.g. > 30 individuals).
#
# Example of how to get PC loadings
#   Rscript  example_data/H3K27ac/get_PCs.R > example_data/H3K27ac/pcs.txt
#
# Using the first 2 PC loadings in the CHT:
#   OUT_FILE=example_data/H3K27ac/cht_results.PCs.txt
#   python CHT/combined_test.py --min_as_counts 10 \
#         --bnb_disp example_data/H3K27ac/cht_bnb_coef.txt \
#         --as_disp example_data/H3K27ac/cht_as_coef.txt \
#         --num_pcs 2 --pc_file example_data/H3K27ac/pcs.txt \
#         $CHT_IN_FILE $OUT_FILE 
#
